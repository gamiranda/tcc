{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0e1b71d",
   "metadata": {},
   "source": [
    "# Crawler e Monta a Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adb9cdc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#bolsonaropresidente\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:189: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "#LulaPresidente\n",
      "1098268451494342658\n",
      "1098268451494342658\n",
      " \n",
      "bolsoladrao\n",
      " \n",
      "luladrao\n",
      "1103687320350412800\n",
      "1125022835175514113\n",
      " \n",
      "Lula\n",
      "1500265842654822401\n",
      "1057720765737504768\n",
      "1266957929468608514\n",
      "1057720765737504768\n",
      "2815705718\n",
      "178912987\n",
      " \n",
      "Bolsonaro\n",
      "1123055030985273344\n",
      "1156751260336500736\n",
      "1123055030985273344\n",
      "53900405\n",
      "1149100684014510080\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import tweepy\n",
    "import time\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "\n",
    "TWITTER_BEARER_TOKEN=\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
    "\n",
    "client = tweepy.Client(bearer_token = TWITTER_BEARER_TOKEN)\n",
    "\n",
    "con = sqlite3.connect(r'base_twitter.db')\n",
    "cur = con.cursor()\n",
    "\n",
    "base_total = pd.read_sql(\"SELECT * FROM tweets\", con = con)\n",
    "\n",
    "for keyword in ['#bolsonaropresidente', '#LulaPresidente', 'bolsoladrao', 'luladrao', 'Lula', 'Bolsonaro']: \n",
    "    \n",
    "    print(keyword)\n",
    "    \n",
    "    tweets_lista = []\n",
    "    colunas = []\n",
    "    \n",
    "    id_usuario = [] \n",
    "    usuario = [] \n",
    "    usuario_tela = [] \n",
    "    data_criacao = [] \n",
    "    n_seguidores = [] \n",
    "    n_seguidos = [] \n",
    "    n_tweets = [] \n",
    "    perfil_fechado = [] \n",
    "    verificado = [] \n",
    "    tem_background_imagem = [] \n",
    "    \n",
    "    tweets = client.search_recent_tweets(query=keyword, \n",
    "                                         tweet_fields=['context_annotations', 'created_at'],\n",
    "                                         user_fields=['public_metrics', 'created_at', 'verified', 'profile_image_url', \n",
    "                                                      'protected'],\n",
    "                                         expansions='author_id', max_results=100)\n",
    "\n",
    "    users = {u[\"id\"]: u for u in tweets.includes['users']}\n",
    "\n",
    "    for tweet in tweets.data:\n",
    "        if users[tweet.author_id]:\n",
    "            user = users[tweet.author_id]\n",
    "            id_usuario.append(user[\"id\"])\n",
    "            usuario.append(user[\"username\"])\n",
    "            usuario_tela.append(user[\"name\"])\n",
    "            data_criacao.append(str(user.created_at))\n",
    "            n_seguidores.append(user.public_metrics['followers_count'])\n",
    "            n_seguidos.append(user.public_metrics['following_count'])\n",
    "            n_tweets.append(user.public_metrics['tweet_count'])\n",
    "            perfil_fechado.append(user.protected)\n",
    "            verificado.append(user.verified)\n",
    "            tem_background_imagem.append(user.profile_image_url)\n",
    "            \n",
    "    df = pd.DataFrame({'ID': id_usuario,\n",
    "                       'Usuario': usuario,\n",
    "                       'Usuario_Tela': usuario_tela,\n",
    "                       'Data_Criacao': data_criacao,\n",
    "                       'N_Seguidores': n_seguidores,\n",
    "                       'N_Seguidos': n_seguidos,\n",
    "                       'N_Tweets': n_tweets,\n",
    "                       'Verificado': verificado,\n",
    "                       'Fechado': perfil_fechado,\n",
    "                       'Background_Img': tem_background_imagem})\n",
    "    \n",
    "    df_tweets = pd.DataFrame()\n",
    "\n",
    "    for i in df['ID']:\n",
    "\n",
    "        tweets_lista = [str(i)]\n",
    "        colunas = ['ID']\n",
    "\n",
    "        j = 1\n",
    "\n",
    "        for aux in tweepy.Paginator(client.get_users_tweets, i, max_results = 100, exclude = \"retweets\", limit = 2):\n",
    "            for v in range(0, 100):\n",
    "                colunas.append(\"tweet_\" + str(j))\n",
    "                j += 1\n",
    "                try:\n",
    "                    tweets_lista.append([aux[0][v]['text']])\n",
    "                except:\n",
    "                    tweets_lista.append(None)\n",
    "        try:\n",
    "            d = dict(zip(colunas, tweets_lista))\n",
    "            df_aux = pd.DataFrame(data = d)\n",
    "            df_tweets = df_tweets.append(df_aux, ignore_index = True)\n",
    "        except:\n",
    "            print(i)\n",
    "        \n",
    "        \n",
    "    df['ID'] = df['ID'].astype(str)\n",
    "    try:\n",
    "        base = pd.merge(df, df_tweets, on = 'ID', how = 'left')\n",
    "    except:\n",
    "        base = df\n",
    "        \n",
    "    base['keyword'] = keyword\n",
    "    \n",
    "    base['troca_sent'] = 0\n",
    "    base['sent_neg'] = 0\n",
    "    base['sent_pos'] = 0\n",
    "    base['sent_neu'] = 0\n",
    "\n",
    "    base['total_vb'] = 0\n",
    "    base['total_adj'] = 0\n",
    "    base['total_adv'] = 0\n",
    "    base['total_pron'] = 0\n",
    "    base['total_hashtag'] = 0\n",
    "\n",
    "    s = SentimentIntensityAnalyzer()\n",
    "\n",
    "    for i in range(0, base.shape[0]):\n",
    "\n",
    "        troca_sent = 0\n",
    "\n",
    "        lista_troca_sent = [\"0\"]\n",
    "\n",
    "        sent_neg = [0]\n",
    "        sent_neu = [0] \n",
    "        sent_pos = [0]\n",
    "\n",
    "        tamanho = 0\n",
    "        total_vb = 0\n",
    "        total_adj = 0\n",
    "        total_adv = 0\n",
    "        total_pron = 0\n",
    "        total_hashtag = 0\n",
    "        total_emoji = 0\n",
    "        total_tk = 0\n",
    "\n",
    "        k = 1\n",
    "\n",
    "        for j in range(10, 210):\n",
    "            aux = str(base.iloc[i, j])\n",
    "\n",
    "            ###### analise sintatica\n",
    "            if aux != None or aux != np.nan or aux != str(nan):\n",
    "                tamanho = tamanho + 1\n",
    "\n",
    "                lista = cont_tipo(aux)\n",
    "\n",
    "                total_vb = total_vb + lista[0]\n",
    "                total_adj = total_adj + lista[1]\n",
    "                total_adv = total_adv + lista[2]\n",
    "                total_pron = total_pron + lista[3]\n",
    "                total_hashtag = total_hashtag + lista[4]\n",
    "\n",
    "            ###### polaridade\n",
    "                pol = s.polarity_scores(aux)\n",
    "\n",
    "                list_pol = [pol['neg'], pol['neu'], pol['pos']]\n",
    "                if list_pol.index(max(list_pol)) == 0:\n",
    "                    sent_neg.append(1)\n",
    "                    lista_troca_sent.append(\"neg\")\n",
    "                elif list_pol.index(max(list_pol)) == 1:\n",
    "                    sent_neu.append(1)\n",
    "                    lista_troca_sent.append(\"neu\")\n",
    "                elif list_pol.index(max(list_pol)) == 2:\n",
    "                    sent_pos.append(1)\n",
    "                    lista_troca_sent.append(\"pos\")\n",
    "\n",
    "                if lista_troca_sent[k] != lista_troca_sent[k - 1]:\n",
    "                    troca_sent += 1\n",
    "\n",
    "                k += 1\n",
    "            else:\n",
    "                print(j)\n",
    "                \n",
    "        base.loc[i, 'total_vb'] = total_vb\n",
    "        base.loc[i, 'total_adj'] = total_adj\n",
    "        base.loc[i, 'total_adv'] = total_adv\n",
    "        base.loc[i, 'total_pron'] = total_pron\n",
    "        base.loc[i, 'total_hashtag'] = total_hashtag\n",
    "        base.loc[i, 'total_emoji'] = total_emoji\n",
    "        base.loc[i, 'total_tk'] = total_tk\n",
    "\n",
    "        base.loc[i, 'sent_neg'] = sum(sent_neg)\n",
    "        base.loc[i, 'sent_neu'] = sum(sent_neu)\n",
    "        base.loc[i, 'sent_pos'] = sum(sent_pos)\n",
    "\n",
    "        base.loc[i, 'troca_sent'] = troca_sent\n",
    "    \n",
    "    base.drop_duplicates(['ID'], inplace = True)\n",
    "    \n",
    "    try:\n",
    "        base_total.drop(['index'], 1, inplace = True)\n",
    "    except:\n",
    "        print(\" \")\n",
    "\n",
    "    base.to_sql(\"tweets\", con = con, if_exists = 'append', index = False)\n",
    "    \n",
    "    time.sleep(15)\n",
    "    \n",
    "con.close()\n",
    "\n",
    "con = sqlite3.connect(r'base_twitter.db')\n",
    "cur = con.cursor()\n",
    "\n",
    "base_total = pd.read_sql(\"SELECT * FROM tweets\", con = con)\n",
    "base_total['media_vb'] = [base_total.loc[i, 'total_vb']/base_total.loc[i, 'total_tk'] for i in range(0, base_total.shape[0])]\n",
    "base_total['media_adj'] = [ base_total.loc[i, 'total_adj']/base_total.loc[i, 'total_tk'] for i in range(0, base_total.shape[0])]\n",
    "base_total['media_adv'] = [ base_total.loc[i, 'total_adv']/base_total.loc[i, 'total_tk'] for i in range(0, base_total.shape[0])]\n",
    "base_total['media_pron'] = [ base_total.loc[i, 'total_pron']/base_total.loc[i, 'total_tk'] for i in range(0, base_total.shape[0])]\n",
    "base_total['media_hashtag'] = [ base_total.loc[i, 'total_hashtag']/base_total.loc[i, 'total_tk'] for i in range(0, base_total.shape[0])]\n",
    "base_total['media_emoji'] = [ base_total.loc[i, 'total_emoji']/base_total.loc[i, 'total_tk'] for i in range(0, base_total.shape[0])]\n",
    "base_total['Ano_Criacao'] = pd.DatetimeIndex(base_total.Data_Criacao).year\n",
    "base_total['Tem_Background_Img'] = [1 if base_total.loc[i, 'Background_Img'] else 0 for i in range(0, base_total.shape[0])]\n",
    "base_total.drop_duplicates(['ID'], inplace = True)\n",
    "base_total.to_sql(\"tweets\", con = con, if_exists=\"replace\", index = False)\n",
    "\n",
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
